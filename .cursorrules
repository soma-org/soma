# Soma Blockchain Development Rules (.cursorrules)

## Project Context
Soma is a Rust-based blockchain node software that provides a Byzantine Fault Tolerant consensus mechanism with validator-based authority. The system processes transactions and maintains state across a distributed network, with clear epoch boundaries for reconfiguration.

## Tool Synchronization Protocol
This file works in conjunction with:
- The memory-bank hierarchy (core/, active/, shared/ folders)
- The .clinerules file for Cline-specific guidance
- The .cursor/rules/*.mdc files for module-specific guidance

### Before Starting with Cursor
1. Check memory-bank files for current context:
   - Review memory-bank/core/activeContext.md for current focus
   - Examine memory-bank/active/[module].md for relevant module status
   - Check memory-bank/shared/tool_transitions.md for context from previous Cline sessions

### During Cursor Sessions
1. Follow patterns from memory-bank/core/systemPatterns.md
2. Reference design documents in memory-bank/design/ folder
3. Apply module-specific rules from .cursor/rules/*.mdc files
4. Document confidence in your implementation (1-10 scale)

### After Completing Work with Cursor
Before switching to Cline, update:
1. memory-bank/active/[module].md with implementation details
2. memory-bank/core/progress.md with completion status and confidence
3. memory-bank/core/systemPatterns.md with any new patterns discovered
4. memory-bank/shared/tool_transitions.md with context for the next Cline session

Example transition note in tool_transitions.md:
```markdown
## 2025-03-07: Cursor to Cline Transition

**Module**: node
**Task**: Implement transaction validation
**Confidence**: 7/10

Implemented basic transaction validation in src/node/transaction.rs and added unit tests in 
src/node/tests/transaction_tests.rs. The implementation follows the validation-first pattern 
from systemPatterns.md, but needs more robust error handling for network failure scenarios.

Files modified:
- src/node/transaction.rs
- src/node/tests/transaction_tests.rs
- src/node/mod.rs

Next steps:
1. Implement proper error handling for network failures
2. Add integration tests for the full validation pipeline
3. Optimize batch validation performance
```

## Tech Stack
Backend:
- Rust (2021 edition)
- Tokio for async runtime
- RocksDB for storage (referenced through traits)
- Tonic/gRPC for network communication
- BCS for serialization

Infrastructure:
- CI/CD with GitHub Actions
- Docker for containerization
- Kubernetes for deployment

## Architectural Patterns and Decisions

### Component Architecture
The system is organized into primary modules that interact with clear boundaries:
- Authority - State management, transaction validation and execution
- Consensus - BFT agreement protocol implementation (Mysticeti)
- Node - Lifecycle and orchestration 
- P2P - Network discovery and state synchronization

### Authority State Management
- The primary state management is through `AuthorityState` and epoch-specific `AuthorityPerEpochStore`
- Clear separation of per-epoch data to handle reconfiguration
- Thread-safe state access via Arc<RwLock<>> pattern
- Example:
```rust
pub struct AuthorityState {
    pub name: AuthorityName,
    pub secret: StableSyncAuthoritySigner,
    epoch_store: ArcSwap<AuthorityPerEpochStore>,
    execution_lock: RwLock<EpochId>,
    committee_store: Arc<CommitteeStore>,
    transaction_manager: Arc<TransactionManager>,
    // Additional fields...
}
```

### Consensus Engine Structure
- Core-based design with round management via ThresholdClock
- Clear commit verification and block production processes
- Leader selection and leader scheduling patterns
- Example:
```rust
pub struct ConsensusAuthority {
    context: Arc<Context>,
    transaction_client: Arc<TransactionClient>,
    synchronizer: Arc<SynchronizerHandle>,
    commit_syncer: CommitSyncer<TonicClient>,
    core_thread_handle: CoreThreadHandle,
    // Additional fields...
}
```

### Transaction Processing Flow
1. **Validation**: Verify transaction signatures and inputs
2. **Execution**: Process the transaction in a temporary store
3. **Commit**: Write results to permanent storage
4. **Notification**: Signal execution completion
5. **State Update**: Update global state with transaction effects

### Epoch Management and Reconfiguration
- Clean epoch transition with state and validator set changes
- Graceful shutdown of previous epoch's services
- Careful state recovery from persistent storage
- Deterministic end-of-epoch detection and handling

### P2P Network Architecture
- Peer discovery with signed node information exchange
- State synchronization for catching up with latest chain state
- Channel management for maintaining persistent peer connections
- Peer balancing for distributed load management

## Async Programming Patterns

### Event-Loop and Handler Pattern
- Central event loops for message processing
- Handler functions for specific message types
- Channel-based communication between components
- Example:
```rust
pub async fn start(mut self) {
    loop {
        tokio::select! {
            now = interval.tick() => {
                self.handle_tick(now.into_std());
            },
            maybe_message = self.mailbox.recv() => {
                if let Some(message) = maybe_message {
                    self.handle_message(message);
                }
            },
            peer_event = self.peer_event_receiver.recv() => {
                self.handle_peer_event(peer_event);
            },
        }
    }
}
```

### Task Spawning and Management
- Use of `JoinSet` for tracking related tasks
- Graceful task cancellation and error handling
- Structured task supervision pattern

### State Protection Patterns
- `RwLock` for shared state that needs concurrent readers
- `Mutex` or `parking_lot::RwLock` for non-async contexts
- `tokio::sync::Mutex` for async contexts requiring exclusive access
- `ArcSwap` for hot-swappable components like epoch store

### Cancellation and Timeout Management
- Proper cancellation propagation through task hierarchies
- Timeout handling for network requests
- Graceful shutdown sequences

## Error Handling

### Error Type Hierarchy
- Module-specific error types that implement `thiserror::Error`
- Clear error categorization (validation, internal, network, etc.)
- Example:
```rust
#[derive(Debug, thiserror::Error)]
pub enum SomaError {
    #[error("epoch has ended: {0}")]
    EpochEnded(EpochId),
    
    #[error("wrong epoch, expected {expected_epoch}, actual {actual_epoch}")]
    WrongEpoch {
        expected_epoch: EpochId,
        actual_epoch: EpochId,
    },
    
    #[error("validator halted at epoch end")]
    ValidatorHaltedAtEpochEnd,
    
    #[error("database error: {0}")]
    DatabaseError(#[from] std::io::Error),
    
    #[error("internal error: {0}")]
    InternalError(String),
}
```

### Error Propagation
- Consistent use of `?` operator for error propagation
- Adding context to errors as they propagate
- Explicit error mapping between modules

## Testing Framework

### Test Structure
- Unit tests focused on component behavior
- Integration tests for component interactions
- End-to-end tests for critical workflows
- Mock implementations for external dependencies

### Async Testing
- Use of `tokio::test` for asynchronous test cases
- Careful management of mock asynchronous behavior

### Test Fixtures
- Centralized test fixtures for common test scenarios
- Factory methods for test object creation
- Helper functions for test setup and verification

## Performance Considerations
- Batch operations where possible
- Use appropriate data structures for access patterns
- Implement proper caching strategies
- Profile performance-critical paths

## Module-Specific Guidelines

### Authority Module
- Implement transaction validation and execution
- Manage state transitions and effects calculation
- Handle epoch boundaries and reconfiguration
- Ensure consistent consensus interaction

### Node Module
- Manage component lifecycle and orchestration
- Handle graceful startup and shutdown sequences
- Coordinate consensus, authority and P2P modules
- Implement service registration and discovery

### Consensus Module
- Implement Byzantine fault-tolerant agreement protocol
- Manage leader selection and block production
- Handle view changes and leader rotation
- Process commits and threshold signatures 

### P2P Module
- Implement peer discovery and connection management
- Provide efficient state synchronization mechanisms
- Handle network partitions and reconnection logic
- Implement message propagation with retry logic

For more detailed guidance, see files in .cursor/rules/ directory and the memory-bank structure.
